{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c870a6ec",
   "metadata": {},
   "source": [
    "# GPT-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f8a9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324f9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pennylane import numpy as np\n",
    "\n",
    "torch.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35664602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: ──RX(0)──RX(2.55)───╭C──────────────╭X──┤ ⟨Z⟩ \n",
      " 1: ──RX(0)──RX(5.12)───╰X──╭C──────────│───┤ ⟨Z⟩ \n",
      " 2: ──RX(0)──RX(0.797)──────╰X──╭C──────│───┤ ⟨Z⟩ \n",
      " 3: ──RX(0)──RX(5.5)────────────╰X──╭C──│───┤     \n",
      " 4: ──RX(0)──RX(4.72)───────────────╰X──╰C──┤     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models import QConv1d\n",
    "\n",
    "ks = 5  # kind of arbitrary, limited by the number of available qubits\n",
    "p = (ks - 1) // 2\n",
    "\n",
    "qconv = QConv1d(kernel_size=ks, out_channels=3, n_qlayers=1, stride=1, padding=p)\n",
    "qconv.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "622958c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "tensor([[[0.310, 0.500, 0.655, 0.777, 0.563, 0.625, 0.045, 0.495],\n",
      "         [0.071, 0.888, 0.020, 0.053, 0.069, 0.692, 0.083, 0.517],\n",
      "         [0.164, 0.454, 0.252, 0.570, 0.795, 0.342, 0.162, 0.544],\n",
      "         [0.562, 0.461, 0.636, 0.300, 0.554, 0.463, 0.585, 0.177]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 4\n",
    "embed_dim = 8\n",
    "x = torch.rand((batch_size, seq_len, embed_dim))\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9bbe70b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "z = qconv(x)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1717b98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:, :, :, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4efc403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 24])\n"
     ]
    }
   ],
   "source": [
    "zc = z.view((1, 4, embed_dim*3))\n",
    "print(zc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31cefb0",
   "metadata": {},
   "source": [
    "1 + (c*w + 2*p - k) / s = w\n",
    "=> (c*w - k) / s = w - 1\n",
    "=> s = (c*w - k) / (w - 1)\n",
    "this has to be an integer.\n",
    "\n",
    "n = (c*w - k) / (w - 1) => n * (w - 1) = c*w - k => k = c*w - n * (w - 1)\n",
    "k = (3 - n) * w + n, 0 < k < 3*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd7ac554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "25\n",
      "18\n",
      "11\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def find_k(w, c):\n",
    "    for n in range(10):\n",
    "        # n = (b*w - k) / (d - 1) => n * (d - 1) = b*w - k => k = b*w - n*w + n\n",
    "        k = c*w - n * (w - 1)\n",
    "        if k < 0:\n",
    "            break\n",
    "        print(k)\n",
    "\n",
    "find_k(embed_dim, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc500c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel size (=no. of qubits): 3\n",
      "stride: 3\n",
      " 0: ──RX(0)──RX(0.386)──╭C──────╭X──┤ ⟨Z⟩ \n",
      " 1: ──RX(0)──RX(5.78)───╰X──╭C──│───┤     \n",
      " 2: ──RX(0)──RX(0.339)──────╰X──╰C──┤     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ks_inv = 3\n",
    "stride_inv = (3 * embed_dim - ks_inv) // (embed_dim - 1)\n",
    "print(f\"kernel size (=no. of qubits): {ks_inv}\")\n",
    "print(f\"stride: {stride_inv}\")\n",
    "qconv_inv = QConv1d(kernel_size=ks_inv, out_channels=1, n_qlayers=1, stride=stride_inv, padding=0)\n",
    "qconv_inv.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62d71b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "y = qconv_inv(zc).view((batch_size, seq_len, -1))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "633ea8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2],\n",
      "          [ 3,  4,  5],\n",
      "          [ 6,  7,  8],\n",
      "          [ 9, 10, 11]],\n",
      "\n",
      "         [[12, 13, 14],\n",
      "          [15, 16, 17],\n",
      "          [18, 19, 20],\n",
      "          [21, 22, 23]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(24).view(1, 2, 4, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0da32923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  3,  6,  9],\n",
      "          [ 1,  4,  7, 10],\n",
      "          [ 2,  5,  8, 11]],\n",
      "\n",
      "         [[12, 15, 18, 21],\n",
      "          [13, 16, 19, 22],\n",
      "          [14, 17, 20, 23]]]])\n",
      "tensor([[[ 0,  3,  6,  9,  1,  4,  7, 10,  2,  5,  8, 11],\n",
      "         [12, 15, 18, 21, 13, 16, 19, 22, 14, 17, 20, 23]]])\n"
     ]
    }
   ],
   "source": [
    "x = x.transpose(-1,-2).view(1,2,3,-1)\n",
    "print(x)\n",
    "x = x.reshape((1,2,12))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fd1ce6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.837e-02,  9.960e-01, -3.804e-01,  6.729e-02,  9.375e-01,\n",
      "          -5.154e-01,  4.412e-03,  8.373e-01, -5.624e-01,  2.919e-02,\n",
      "           7.044e-01, -5.332e-01, -1.181e-01,  5.778e-01, -3.474e-01,\n",
      "          -2.614e-02,  6.345e-01, -4.119e-01, -3.615e-02,  7.000e-01,\n",
      "          -8.884e-02, -2.691e-01,  8.537e-01, -4.654e-01],\n",
      "         [-2.798e-02,  9.960e-01, -1.521e-01, -3.674e-01,  9.908e-01,\n",
      "          -8.172e-01, -2.644e-02,  6.025e-01, -6.152e-02,  1.809e-02,\n",
      "           6.916e-01, -9.328e-02, -3.616e-02,  9.941e-01, -1.496e-01,\n",
      "          -1.908e-02,  9.940e-01, -6.946e-01, -4.321e-02,  7.465e-01,\n",
      "          -1.226e-01, -2.772e-01,  8.142e-01, -4.589e-01],\n",
      "         [-5.107e-02,  9.960e-01, -2.425e-01,  9.954e-03,  9.767e-01,\n",
      "          -4.985e-01,  4.539e-02,  8.794e-01, -2.882e-01, -5.328e-02,\n",
      "           8.934e-01, -5.421e-01, -1.886e-01,  8.099e-01, -6.228e-01,\n",
      "          -7.285e-04,  5.951e-01, -2.446e-01, -7.682e-02,  7.031e-01,\n",
      "          -1.699e-01, -2.843e-01,  9.476e-01, -5.549e-01],\n",
      "         [ 3.846e-02,  9.960e-01, -5.977e-01, -6.114e-02,  8.237e-01,\n",
      "          -4.256e-01,  3.345e-03,  7.798e-01, -5.130e-01, -1.575e-02,\n",
      "           7.269e-01, -2.707e-01,  1.525e-02,  8.030e-01, -4.767e-01,\n",
      "          -9.751e-02,  8.114e-01, -4.209e-01, -2.494e-01,  7.820e-01,\n",
      "          -4.835e-01, -1.032e-01,  7.550e-01, -1.935e-01]]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.837e-02, -2.798e-02, -5.107e-02,  3.846e-02],\n",
       "         [ 9.960e-01,  9.960e-01,  9.960e-01,  9.960e-01],\n",
       "         [-3.804e-01, -1.521e-01, -2.425e-01, -5.977e-01],\n",
       "         [ 6.729e-02, -3.674e-01,  9.954e-03, -6.114e-02],\n",
       "         [ 9.375e-01,  9.908e-01,  9.767e-01,  8.237e-01],\n",
       "         [-5.154e-01, -8.172e-01, -4.985e-01, -4.256e-01],\n",
       "         [ 4.412e-03, -2.644e-02,  4.539e-02,  3.345e-03],\n",
       "         [ 8.373e-01,  6.025e-01,  8.794e-01,  7.798e-01],\n",
       "         [-5.624e-01, -6.152e-02, -2.882e-01, -5.130e-01],\n",
       "         [ 2.919e-02,  1.809e-02, -5.328e-02, -1.575e-02],\n",
       "         [ 7.044e-01,  6.916e-01,  8.934e-01,  7.269e-01],\n",
       "         [-5.332e-01, -9.328e-02, -5.421e-01, -2.707e-01],\n",
       "         [-1.181e-01, -3.616e-02, -1.886e-01,  1.525e-02],\n",
       "         [ 5.778e-01,  9.941e-01,  8.099e-01,  8.030e-01],\n",
       "         [-3.474e-01, -1.496e-01, -6.228e-01, -4.767e-01],\n",
       "         [-2.614e-02, -1.908e-02, -7.285e-04, -9.751e-02],\n",
       "         [ 6.345e-01,  9.940e-01,  5.951e-01,  8.114e-01],\n",
       "         [-4.119e-01, -6.946e-01, -2.446e-01, -4.209e-01],\n",
       "         [-3.615e-02, -4.321e-02, -7.682e-02, -2.494e-01],\n",
       "         [ 7.000e-01,  7.465e-01,  7.031e-01,  7.820e-01],\n",
       "         [-8.884e-02, -1.226e-01, -1.699e-01, -4.835e-01],\n",
       "         [-2.691e-01, -2.772e-01, -2.843e-01, -1.032e-01],\n",
       "         [ 8.537e-01,  8.142e-01,  9.476e-01,  7.550e-01],\n",
       "         [-4.654e-01, -4.589e-01, -5.549e-01, -1.935e-01]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(zc)\n",
    "zc.transpose(-1, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0fcc08",
   "metadata": {},
   "source": [
    "### vectorize input to CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb071dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "tensor([[[0.361, 0.553, 0.733, 0.886, 0.842, 0.999, 0.695, 0.516],\n",
      "         [0.103, 0.035, 0.632, 0.898, 0.592, 0.010, 0.843, 0.252],\n",
      "         [0.948, 0.475, 0.857, 0.788, 0.344, 0.803, 0.183, 0.217],\n",
      "         [0.851, 0.826, 0.503, 0.877, 0.742, 0.920, 0.205, 0.533]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 4\n",
    "embed_dim = 8\n",
    "x = torch.rand((batch_size, seq_len, embed_dim))\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee8bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 2\n",
    "kernel_size = 3\n",
    "padding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a8c228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "out_dim: 3\n",
      "\n",
      "tensor([0.361, 0.553, 0.733])\n",
      "tensor([0.733, 0.886, 0.842])\n",
      "tensor([0.842, 0.999, 0.695])\n",
      "---\n",
      "tensor([0.103, 0.035, 0.632])\n",
      "tensor([0.632, 0.898, 0.592])\n",
      "tensor([0.592, 0.010, 0.843])\n",
      "---\n",
      "tensor([0.948, 0.475, 0.857])\n",
      "tensor([0.857, 0.788, 0.344])\n",
      "tensor([0.344, 0.803, 0.183])\n",
      "---\n",
      "tensor([0.851, 0.826, 0.503])\n",
      "tensor([0.503, 0.877, 0.742])\n",
      "tensor([0.742, 0.920, 0.205])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "out_dim = int((embed_dim + 2 * padding - kernel_size) / stride) + 1\n",
    "z = F.pad(x, (padding, padding), \"constant\", 0)\n",
    "print(z.shape)\n",
    "print(f\"out_dim: {out_dim}\\n\")\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len):\n",
    "        for k in range(0, out_dim):\n",
    "            k_start = k*stride\n",
    "            k_end = k_start + kernel_size\n",
    "            z_slice  = z[i, j, k_start:k_end]\n",
    "            print(z_slice)\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af9bc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.361, 0.553, 0.733],\n",
       "         [0.103, 0.035, 0.632],\n",
       "         [0.948, 0.475, 0.857],\n",
       "         [0.851, 0.826, 0.503]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.arange(0, kernel_size)\n",
    "x[:,:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa58bb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [2 3 4]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "idx = np.expand_dims(np.arange(kernel_size), 0) + np.expand_dims(np.arange(out_dim) * stride, 0).T\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb9c74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.361, 0.553, 0.733],\n",
       "          [0.733, 0.886, 0.842],\n",
       "          [0.842, 0.999, 0.695]],\n",
       "\n",
       "         [[0.103, 0.035, 0.632],\n",
       "          [0.632, 0.898, 0.592],\n",
       "          [0.592, 0.010, 0.843]],\n",
       "\n",
       "         [[0.948, 0.475, 0.857],\n",
       "          [0.857, 0.788, 0.344],\n",
       "          [0.344, 0.803, 0.183]],\n",
       "\n",
       "         [[0.851, 0.826, 0.503],\n",
       "          [0.503, 0.877, 0.742],\n",
       "          [0.742, 0.920, 0.205]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8dfcf6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import QConv1d\n",
    "\n",
    "qconv = QConv1d(kernel_size, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "067f02be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 6, 3])\n",
      "tensor([[[[0.973, 0.808, 0.797],\n",
      "          [0.919, 0.795, 0.746],\n",
      "          [0.976, 0.968, 0.945],\n",
      "          [0.830, 0.752, 0.750],\n",
      "          [0.890, 0.896, 0.883],\n",
      "          [0.958, 0.980, 0.958]],\n",
      "\n",
      "         [[0.973, 0.987, 0.964],\n",
      "          [0.902, 0.918, 0.838],\n",
      "          [0.990, 0.953, 0.945],\n",
      "          [0.964, 0.848, 0.842],\n",
      "          [0.799, 0.861, 0.780],\n",
      "          [0.802, 0.997, 0.802]],\n",
      "\n",
      "         [[0.744, 0.995, 0.740],\n",
      "          [0.839, 0.841, 0.753],\n",
      "          [0.995, 0.680, 0.679],\n",
      "          [0.911, 0.782, 0.778],\n",
      "          [0.958, 0.965, 0.957],\n",
      "          [0.872, 0.860, 0.855]],\n",
      "\n",
      "         [[0.973, 0.766, 0.763],\n",
      "          [0.710, 0.749, 0.554],\n",
      "          [0.903, 0.919, 0.889],\n",
      "          [0.810, 0.673, 0.549],\n",
      "          [0.963, 0.913, 0.906],\n",
      "          [0.698, 0.670, 0.532]]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = qconv(x)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b819b77b",
   "metadata": {},
   "source": [
    "## FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a16a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FeedForwardQuantum\n",
    "\n",
    "embed_dim = 8\n",
    "n_qubits = 5\n",
    "n_qlayers = 1\n",
    "boom_factor = 4\n",
    "ff = FeedForwardQuantum(embed_dim, boom_factor=boom_factor, n_qubits=n_qubits, n_qlayers=n_qlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b182954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 4, 8])\n",
      "output: torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 4\n",
    "embed_dim = 8\n",
    "x = torch.rand((batch_size, seq_len, embed_dim))\n",
    "\n",
    "print(\"input:\", x.shape)\n",
    "xff = ff.forward(x)\n",
    "print(\"output:\", xff.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535403e",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f242bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "tensor([[[0.964, 0.776, 0.797, 0.132, 0.005, 0.225, 0.763, 0.818],\n",
      "         [0.998, 0.020, 0.987, 0.329, 0.430, 0.608, 0.548, 0.698],\n",
      "         [0.829, 0.442, 0.601, 0.259, 0.688, 0.809, 0.910, 0.808],\n",
      "         [0.446, 0.084, 0.925, 0.620, 0.958, 0.173, 0.355, 0.116]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 4\n",
    "embed_dim = 8\n",
    "x = torch.rand((batch_size, seq_len, embed_dim))\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b199fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MultiHeadAttentionQuantum\n",
    "\n",
    "n_heads = 2\n",
    "n_qubits = 5\n",
    "n_qlayers = 1\n",
    "n_heads = 4\n",
    "\n",
    "attn = MultiHeadAttentionQuantum(embed_dim, n_heads, n_qubits, n_qlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec263ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of attention: torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "out = attn(x)\n",
    "print(\"output of attention:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c36ea",
   "metadata": {},
   "source": [
    "## Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41632043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TransformerBlockQuantum\n",
    "\n",
    "transformer = TransformerBlockQuantum(embed_dim, n_heads=n_heads, n_qubits=n_qubits, n_qlayers=n_qlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50e11a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer block output: torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "x_tf = transformer.forward(x)\n",
    "print(\"transformer block output:\", x_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9483c5",
   "metadata": {},
   "source": [
    "## GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f02f1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GPT2\n",
    "\n",
    "embed_dim = 8\n",
    "batch_size = 1\n",
    "max_seq_len = 16\n",
    "src_vocab = 8\n",
    "tgt_vocab = 4 \n",
    "n_tlayers = 2\n",
    "n_heads = 4\n",
    "\n",
    "gpt2 = GPT2(embed_dim=embed_dim,\n",
    "            src_vocab=src_vocab,\n",
    "            tgt_vocab=tgt_vocab,\n",
    "            n_heads=n_heads,\n",
    "            n_tlayers=n_tlayers,\n",
    "            max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c93966d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 5, 3, 7, 3, 7, 1, 7, 6, 4, 4, 2, 4, 4, 4, 6]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = np.random.choice(src_vocab, (batch_size, max_seq_len))\n",
    "token_ids = torch.tensor(token_ids)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3159def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.39 ms ± 126 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "GPT-2 output: torch.Size([1, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "%timeit out = gpt2(token_ids)\n",
    "print(\"GPT-2 output:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833898a",
   "metadata": {},
   "source": [
    "## GPT-Q Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a218f697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of models failed: Traceback (most recent call last):\n",
      "  File \"/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: forward() requires a code object with 1 free vars, not 0\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from models import GPTQ\n",
    "\n",
    "n_qlayers = 1\n",
    "q_device = \"default.qubit\"\n",
    "\n",
    "gptq = GPTQ(embed_dim=embed_dim,\n",
    "            src_vocab=src_vocab,\n",
    "            tgt_vocab=tgt_vocab,\n",
    "            n_heads=n_heads,\n",
    "            n_tlayers=n_tlayers,\n",
    "            max_seq_len=max_seq_len,\n",
    "            n_qlayers=n_qlayers,\n",
    "            q_device=q_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ae30c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.99 s ± 404 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "GPT-Q output: torch.Size([1, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "%timeit out = gptq(token_ids)\n",
    "print(\"GPT-Q output:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53ce97bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptq.attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377301e",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8dcbf381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import make_padding_mask, make_subsequent_mask, make_lookahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a1d1abea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 4, 2, 7],\n",
      "        [6, 7, 5, 2]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_seq_len = 4\n",
    "token_ids = np.random.choice(src_vocab, (batch_size, max_seq_len))\n",
    "token_ids = torch.tensor(token_ids)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae24c391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_padding_mask(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "38650604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x141bd6400>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEzCAYAAACmDxGBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAARfElEQVR4nO3cYawddZnH8e/P2wK6EMC2gVoK6Ep0lVXAGxZlsyGgEYkBEzGLmygYTNWVVRNfLGhSI29WzUY3LkZCgAjGIC64eiUlBBaMmg2Fa7cUKCKVN1Drgq0WGhW35NkXd9R/j6f0tmfOuRf4fpKT+5+Z/53n6dDz65yZM6SqkCTNeclCNyBJi4mhKEkNQ1GSGoaiJDUMRUlqGIqS1BgpFJO8PMntSR7pfh65l3nPJtnYvWZGqSlJ45RRvqeY5AvAjqr6XJJLgSOr6p+HzNtVVYeO0KckTcSoofgwcEZVbUuyEvh+Vb1myDxDUdLzwqjXFI+qqm3d+BfAUXuZd0iS2SR3J3nXiDUlaWyW7GtCkjuAo4ds+nS7UFWVZG+nncdV1dYkrwLuTHJ/Vf1sSK01wBqAv3hZ3vTaVx+0zz/Ai8VPN71soVuQnjee5le/rKoVB/K7E/n4PPA7XwNuqaqbnmve9BsPqXtuW33Avb3QvP0VJy10C9Lzxh1104+ravpAfnfUj88zwIXd+ELgu4MTkhyZ5OBuvBw4Hdg8Yl1JGotRQ/FzwNuSPAK8tVsmyXSSq7s5fwXMJrkPuAv4XFUZipIWpX1eU3wuVbUdOGvI+lngg934v4G/HqWOJE2KT7RIUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUqOXUExydpKHk2xJcumQ7QcnubHbvj7J8X3UlaS+jRyKSaaArwDvAF4HvDfJ6wamXQz8qqpeDXwJ+PyodSVpHPo4UzwV2FJVj1bV74FvAucNzDkPuK4b3wSclSQ91JakXvURiquAx5rlx7t1Q+dU1W5gJ7Csh9qS1KtFdaMlyZoks0lmn9z+7EK3I+lFqI9Q3AqsbpaP6dYNnZNkCXA4sH1wR1V1VVVNV9X0imVTPbQmSfunj1C8FzghySuTHARcAMwMzJkBLuzG5wN3VlX1UFuSerVk1B1U1e4klwC3AVPAtVX1YJLLgdmqmgGuAb6eZAuwg7nglKRFZ+RQBKiqdcC6gXVrm/HvgPf0UUuSxmlR3WiRpIVmKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJanRSygmOTvJw0m2JLl0yPaLkjyZZGP3+mAfdSWpb0tG3UGSKeArwNuAx4F7k8xU1eaBqTdW1SWj1pOkcerjTPFUYEtVPVpVvwe+CZzXw34laeL6CMVVwGPN8uPdukHvTrIpyU1JVvdQV5J6N/LH53n6HnBDVT2T5EPAdcCZg5OSrAHWABy7alKtPT/c9vONC93CovL2V5y00C3oBaqPM8WtQHvmd0y37o+qantVPdMtXg28adiOquqqqpququkVy6Z6aE2S9k8foXgvcEKSVyY5CLgAmGknJFnZLJ4LPNRDXUnq3cifUatqd5JLgNuAKeDaqnowyeXAbFXNAB9Lci6wG9gBXDRqXUkah14u3FXVOmDdwLq1zfgy4LI+aknSOPlEiyQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1egnFJNcmeSLJA3vZniRfTrIlyaYkp/RRV5L61teZ4teAs59j+zuAE7rXGuCrPdWVpF71EopV9QNgx3NMOQ+4vubcDRyRZGUftSWpT5O6prgKeKxZfrxbJ0mLyqK60ZJkTZLZJLNPbn92oduR9CI0qVDcCqxulo/p1u2hqq6qqumqml6xbGpCrUnSn0wqFGeA93d3oU8DdlbVtgnVlqR5W9LHTpLcAJwBLE/yOPAZYClAVV0JrAPOAbYAvwE+0EddSepbL6FYVe/dx/YCPtpHLUkap0V1o0WSFpqhKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpEYvoZjk2iRPJHlgL9vPSLIzycbutbaPupLUtyU97edrwBXA9c8x54dV9c6e6knSWPRyplhVPwB29LEvSVpIk7ym+OYk9yW5NcnrJ1hXkuatr4/P+7IBOK6qdiU5B/gOcMLgpCRrgDUAx66aVGt6Prrt5xsXuoVF5e2vOGmhW3jBmMiZYlU9VVW7uvE6YGmS5UPmXVVV01U1vWLZ1CRak6Q9TCQUkxydJN341K7u9knUlqT90ctn1CQ3AGcAy5M8DnwGWApQVVcC5wMfSbIb+C1wQVVVH7UlqU+9hGJVvXcf269g7is7krSo+USLJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDVGDsUkq5PclWRzkgeTfHzInCT5cpItSTYlOWXUupI0Dkt62Mdu4JNVtSHJYcCPk9xeVZubOe8ATuhefwN8tfspSYvKyGeKVbWtqjZ046eBh4BVA9POA66vOXcDRyRZOWptSepbr9cUkxwPnAysH9i0CnisWX6cPw9OSVpwvYVikkOBm4FPVNVTB7iPNUlmk8w+uf3ZvlqTpHnrJRSTLGUuEL9RVd8eMmUrsLpZPqZbt4equqqqpqtqesWyqT5ak6T90sfd5wDXAA9V1Rf3Mm0GeH93F/o0YGdVbRu1tiT1rY+7z6cD7wPuT7KxW/cp4FiAqroSWAecA2wBfgN8oIe6ktS7kUOxqn4EZB9zCvjoqLUkadx8okWSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGiOHYpLVSe5KsjnJg0k+PmTOGUl2JtnYvdaOWleSxmFJD/vYDXyyqjYkOQz4cZLbq2rzwLwfVtU7e6gnSWMz8pliVW2rqg3d+GngIWDVqPuVpIXQ6zXFJMcDJwPrh2x+c5L7ktya5PV91pWkvvTx8RmAJIcCNwOfqKqnBjZvAI6rql1JzgG+A5wwZB9rgDUAx67qrTXpBe+2n29c6BYWlamVB/67vZwpJlnKXCB+o6q+Pbi9qp6qql3deB2wNMnyIfOuqqrpqppesWyqj9Ykab/0cfc5wDXAQ1X1xb3MObqbR5JTu7rbR60tSX3r4zPq6cD7gPuTbOzWfQo4FqCqrgTOBz6SZDfwW+CCqqoeaktSr0YOxar6EZB9zLkCuGLUWpI0bj7RIkkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEmNkUMxySFJ7klyX5IHk3x2yJyDk9yYZEuS9UmOH7WuJI1DH2eKzwBnVtUbgZOAs5OcNjDnYuBXVfVq4EvA53uoK0m9GzkUa86ubnFp96qBaecB13Xjm4CzkmTU2pLUt16uKSaZSrIReAK4varWD0xZBTwGUFW7gZ3Asj5qS1KfegnFqnq2qk4CjgFOTXLigewnyZoks0lmn9z+bB+tSdJ+6fXuc1X9GrgLOHtg01ZgNUCSJcDhwPYhv39VVU1X1fSKZVN9tiZJ89LH3ecVSY7oxi8F3gb8ZGDaDHBhNz4fuLOqBq87StKCW9LDPlYC1yWZYi5kv1VVtyS5HJitqhngGuDrSbYAO4ALeqgrSb0bORSrahNw8pD1a5vx74D3jFpLksbNJ1okqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqTFyKCY5JMk9Se5L8mCSzw6Zc1GSJ5Ns7F4fHLWuJI3Dkh728QxwZlXtSrIU+FGSW6vq7oF5N1bVJT3Uk6SxGTkUq6qAXd3i0u5Vo+5XkhZCL9cUk0wl2Qg8AdxeVeuHTHt3kk1Jbkqyuo+6ktS3zJ3o9bSz5AjgP4F/qqoHmvXLgF1V9UySDwF/X1VnDvn9NcCabvFE4IHBOQtgOfDLhW4C+xhkH3uyjz29pqoOO5Bf7DUUAZKsBX5TVf+6l+1TwI6qOnwf+5mtqulemzsA9mEf9vHi6qOPu88rujNEkrwUeBvwk4E5K5vFc4GHRq0rSePQx93nlcB13RngS4BvVdUtSS4HZqtqBvhYknOB3cAO4KIe6kpS7/q4+7wJOHnI+rXN+DLgsv3c9VUjttYX+9iTfezJPvb0vO+j92uKkvR85mN+ktRYNKGY5OVJbk/ySPfzyL3Me7Z5XHCmx/pnJ3k4yZYklw7ZfnCSG7vt65Mc31ft/exjIo9MJrk2yRNJhn4tKnO+3PW5KckpC9DDGUl2Nsdi7bB5PfSxOsldSTZ3j7J+fMicSRyP+fQx9mMyz0d7x/5+GdsjxlW1KF7AF4BLu/GlwOf3Mm/XGGpPAT8DXgUcBNwHvG5gzj8CV3bjC5h7bHEh+rgIuGIC/z3+DjgFeGAv288BbgUCnAasX4AezgBumcCxWAmc0o0PA3465L/LJI7HfPoY+zHp/oyHduOlwHrgtIE5k3i/zKeP/X6/LJozReA84LpufB3wrgnWPhXYUlWPVtXvgW92/bTa/m4CzkqSBehjIqrqB8x9U2BvzgOurzl3A0cMfPVqEj1MRFVtq6oN3fhp5r5Stmpg2iSOx3z6GLvuz7ivR3vH/n6ZZx/7bTGF4lFVta0b/wI4ai/zDkkym+TuJO/qqfYq4LFm+XH+/C/bH+dU1W5gJ7Csp/r70wcsjkcm59vruL25+/h0a5LXj7tY9zHwZObOSloTPR7P0QdM4Jhk34/2TuL9MpZHjCcaiknuSPLAkNceZ0M1d967t8Q/rua+qf4PwL8l+ctx973IfA84vqreANzOn/41fjHawNzfhzcC/w58Z5zFkhwK3Ax8oqqeGmetEfqYyDGpqmer6iTgGODUJCeOo04Pfez3+2WioVhVb62qE4e8vgv87x8+bnQ/n9jLPrZ2Px8Fvs+Q70gegK1A+y/IMd26oXOSLAEOB7b3UHu/+qiq7VX1TLd4NfCmnnuYr/kcs7Gqqqf+8PGpqtYBS5MsH0etzP1v8W4GvlFV3x4yZSLHY199TPKYdDV+DdwFnD2waRLvl332cSDvl8X08XkGuLAbXwh8d3BCkiOTHNyNlwOnA5t7qH0vcEKSVyY5iLkLw4N3ttv+zgfu7M5o+7TPPrJ4HpmcAd7f3XU9DdjZXP6YiCRH/+E6VZJTmfv73Psbr6txDfBQVX1xL9PGfjzm08ckjknm8WgvE3i/zKePA3q/9H1H6EBfzF1v+C/gEeAO4OXd+mng6m78FuB+5u7K3g9c3GP9c5i7m/cz4NPdusuBc7vxIcB/AFuAe4BXjek47KuPfwEe7I7BXcBrx9THDcA24P+Yuz52MfBh4MP1pzt/X+n6vB+YXoAeLmmOxd3AW8Z0LP6Wucs5m4CN3eucBTge8+lj7McEeAPwP10fDwBrh/w9Hfv7ZZ597Pf7xSdaJKmxmD4+S9KCMxQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWr8P95vBEaiNsSsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(make_subsequent_mask(max_seq_len)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cd7fff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 0, 0],\n",
      "        [1, 2, 3, 4, 0]], dtype=torch.int32)\n",
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from utils import make_lookahead_mask, make_src_mask\n",
    "\n",
    "\n",
    "token_ids = np.array([\n",
    "    [1, 2, 3, 0, 0],\n",
    "    [1, 2, 3, 4, 0]\n",
    "], dtype=np.int32)\n",
    "token_ids = torch.from_numpy(token_ids)\n",
    "\n",
    "print(token_ids)\n",
    "print(make_src_mask(token_ids.size(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d00099",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "054839e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b7fd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = IMDB(split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a7d5265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter), len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84da71ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.']\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for label, line in train_iter:\n",
    "    train_data.append(line)\n",
    "print(train_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9da8cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "\n",
    "vocab_size = 300\n",
    "min_frequency = 2\n",
    "special_tokens = [\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ]\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoders = decoders.ByteLevel()\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    min_frequency=min_frequency,\n",
    "    special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train_from_iterator(train_data, trainer=trainer)\n",
    "tokenizer.save(\"gptq.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f40a6dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[225, 44, 73, 287, 83, 268, 282, 80, 72]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello world\").ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad77012",
   "metadata": {},
   "source": [
    "## Fit IMDb Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8757d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import IMDbDataModule\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "max_seq_len = 16\n",
    "n_examples_max = 16\n",
    "#n_examples_max = 8 # for quick tests\n",
    "#n_examples_max = None\n",
    "\n",
    "dm = IMDbDataModule(val_split=0.2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    max_seq_length=max_seq_len,\n",
    "                    n_examples_max=n_examples_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    gpus=AVAIL_GPUS,\n",
    "    log_every_n_steps=2,\n",
    "    progress_bar_refresh_rate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89b229b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GPT2, IMDbClassifier\n",
    "\n",
    "embed_dim = 8\n",
    "vocab_size = 2000\n",
    "n_tlayers = 1\n",
    "n_heads = 2\n",
    "lr = 1e-3\n",
    "\n",
    "classifier = IMDbClassifier(embed_dim=embed_dim,\n",
    "                       vocab_size=vocab_size,\n",
    "                       n_heads=n_heads,\n",
    "                       n_tlayers=n_tlayers,\n",
    "                       max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f55ed28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.distributed:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.distributed:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.distributed:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.core.lightning:\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | wte     | Embedding  | 16.0 K\n",
      "1 | wpe     | Embedding  | 128   \n",
      "2 | dropout | Dropout    | 0     \n",
      "3 | ln_f    | LayerNorm  | 16    \n",
      "4 | h       | ModuleList | 872   \n",
      "5 | out     | Linear     | 16    \n",
      "---------------------------------------\n",
      "17.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.0 K    Total params\n",
      "0.068     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e25ea49abd548929b20dae27efcea97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(classifier, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "558a808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), \"imdb_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f630c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "545b5ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-95f951a17b991dc9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-95f951a17b991dc9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62732c",
   "metadata": {},
   "source": [
    "## IMBd Classifier Quantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ac01678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GPTQ, IMDbClassifierQuantum\n",
    "\n",
    "n_qlayers = 1\n",
    "q_device = 'qulacs.simulator'\n",
    "\n",
    "classifier_quantum = IMDbClassifierQuantum(embed_dim=embed_dim,\n",
    "                       vocab_size=vocab_size,\n",
    "                       n_heads=n_heads,\n",
    "                       n_tlayers=n_tlayers,\n",
    "                       max_seq_len=max_seq_len,\n",
    "                       q_device=q_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ce7635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.distributed:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.distributed:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.distributed:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.core.lightning:\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | wte     | Embedding  | 16.0 K\n",
      "1 | wpe     | Embedding  | 128   \n",
      "2 | dropout | Dropout    | 0     \n",
      "3 | ln_f    | LayerNorm  | 16    \n",
      "4 | h       | ModuleList | 51    \n",
      "5 | out     | Linear     | 16    \n",
      "---------------------------------------\n",
      "16.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.2 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c101fe2abb424e85e018976c4aa7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:147: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:240.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/disipio/development/gpt-q/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(classifier_quantum, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb4e4b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier_quantum.state_dict(), \"imdb_classifier_quantum.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d91aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b50d3",
   "metadata": {},
   "source": [
    "## Fit Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "586234df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "487b067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2000\n",
    "max_seq_len = 64\n",
    "n_layers = 1\n",
    "n_heads = 2\n",
    "lr = 1e-3\n",
    "\n",
    "model = LanguageModel(embed_dim=embed_dim,\n",
    "                       vocab_size=vocab_size,\n",
    "                       n_heads=n_heads,\n",
    "                       n_layers=n_layers,\n",
    "                       max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f62b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
